{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Prosper Propensity Models (BASIC)\n",
    "\n",
    "This notebook illustrates the use of a subset of Prosper Propensity Models based on a simple feature set called basic-geo. These models may be used for realtime inference and batch transform jobs. \n",
    "\n",
    "## Input Variables\n",
    "\n",
    "**Gender** (Integer, 0-1)    \n",
    "0 = Female  \n",
    "1 = Male \n",
    "  \n",
    "**Age Range** (Integer, 1-6)  \n",
    "1 = 18-24   \n",
    "2 = 25-34  \n",
    "3 = 35-44  \n",
    "4 = 45-54  \n",
    "5 = 55-64  \n",
    "6 = 65+  \n",
    "  \n",
    "**Household Income Range (USD)** (Integer, 0-24)  \n",
    " 0 = Less than 10,000  \n",
    " 1 = 10,000 to 14,999  \n",
    " 2 = 15,000 to 19,999  \n",
    " 3 = 20,000 to 24,999  \n",
    " 4 = 25,000 to 29,999  \n",
    " 5 = 30,000 to 34,999  \n",
    " 6 = 35,000 to 39,999  \n",
    " 7 = 40,000 to 44,999  \n",
    " 8 = 45,000 to 49,999  \n",
    " 9 = 50,000 to 54,999  \n",
    " 10 = 55,000 to 59,999  \n",
    " 11 = 60,000 to 64,999  \n",
    " 12 = 65,000 to 69,999  \n",
    " 13 = 70,000 to 74,999  \n",
    " 14 = 75,000 to 79,999  \n",
    " 15 = 80,000 to 84,999  \n",
    " 16 = 85,000 to 89,999  \n",
    " 17 = 90,000 to 94,999  \n",
    " 18 = 95,000 to 99,999  \n",
    " 19 = 100,000 to 109,999  \n",
    " 20 = 110,000 to 119,999  \n",
    " 21 = 120,000 to 129,999  \n",
    " 22 = 130,000 to 139,999  \n",
    " 23 = 140,000 to 149,999  \n",
    " 24 = 150,000 or more  \n",
    "\n",
    "## Result\n",
    "\n",
    "The Prosper Model returns a score between 0 and 1 that represents the probability that the person has the target attribute.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sm\n",
    "\n",
    "# Specify the ARN of the model package you will be using. You can get this\n",
    "# from the sagemaker console after you subscribe to the model package.\n",
    "model_package_arn = 'arn goes here'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "\n",
    "# Realtime Inference Example\n",
    "1. Deploy model to endpoint\n",
    "2. Submit inference requests to the endpoint\n",
    "3. Delete the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model to an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_package_arn: arn:aws:sagemaker:us-east-2:214666064132:model-package/trends-styles-important-basic\n",
      "endpoint_name: prosper-test-endpoint-20191124202027\n",
      "-------------------------------------------------------------------------------------!\n",
      "Endpoint prosper-test-endpoint-20191124202027 is now in service\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Set endpoint name. You may replace the following with any name you like. \n",
    "endpoint_name = 'prosper-test-endpoint-' + time.strftime(\"%Y%m%d%H%M%S\", time.localtime())\n",
    "\n",
    "# print the critical names defined at this point\n",
    "print(f'model_package_arn: {model_package_arn}')\n",
    "print(f'endpoint_name: {endpoint_name}')\n",
    "\n",
    "# Create model from model package\n",
    "model = sm.ModelPackage(\n",
    "            role=sm.get_execution_role(),\n",
    "            model_package_arn=model_package_arn,\n",
    "            sagemaker_session=sm.Session())\n",
    "\n",
    "# Deploy the model to an endpoint. Be sure to delete the endpoint when you are finished with it.\n",
    "# By default, this method waits until the endpoint is deployed. This could take a while.\n",
    "# To have the API return immediately, set the wait parameter to false. Note, however, that you\n",
    "# cannot submit a request to the endpoint until it is in service. The easy way to check the status is\n",
    "# by using the sagemaker console.\n",
    "model.deploy(1, 'ml.m4.xlarge', endpoint_name=endpoint_name)\n",
    "\n",
    "print(f'\\nEndpoint {endpoint_name} is now in service')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit a realtime inference request to the endpoint created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request Data: 0,1,14\n",
      "Response (probability): 0.7214754223823547\n"
     ]
    }
   ],
   "source": [
    "# create predictor for endpoint created above\n",
    "predictor = sm.predictor.RealTimePredictor(endpoint_name, content_type='text/csv')\n",
    "\n",
    "# Sample request\n",
    "# Female\n",
    "gender = 0\n",
    "# Age Range: 18-24\n",
    "age_range = 1\n",
    "# Household Income: $75,000 to $79,999\n",
    "household_income_range = 14\n",
    "\n",
    "# format request data as comma-delimited string\n",
    "request_data = f'{gender},{age_range},{household_income_range}'\n",
    "print(f'Request Data: {request_data}')\n",
    "\n",
    "# Submit the request to the endpoint.\n",
    "# By default, the result is returned as a sequence of bytes. We decode it as utf-8 string.\n",
    "# Note that there are parameters available for serializing and deserializing input and output data.\n",
    "result = predictor.predict(request_data).decode('utf-8')\n",
    "\n",
    "# print the result. This is the probability that the person with the requested parameters are in the\n",
    "# target class.\n",
    "print(f'Response (probability): {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.session.Session().delete_endpoint(endpoint_name)\n",
    "sm.session.Session().delete_endpoint_config(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "***\n",
    "    \n",
    "# Batch Transform Example\n",
    "1. Prepare the input file\n",
    "2. Create the transform job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the input file\n",
    "Upload the sample input file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform input: s3://sagemaker-us-east-2-214666064132/prosper-sample-data/basic_test_data.csv\n"
     ]
    }
   ],
   "source": [
    "# A sample input file (prosper_data/basic_test_data.csv) is provided with this notebook.\n",
    "# The transform job expects its input file to to live in S3.  We upload the file\n",
    "# to the default bucket with key_prefix of prosper-sample-data. You\n",
    "# can upload to any bucket and key you like if you specify the bucket and key_prefix parameters.\n",
    "\n",
    "transform_input = sm.Session().upload_data('prosper_data/basic_test_data.csv', key_prefix='prosper-sample-data')\n",
    "print(f'transform input: {transform_input}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the transform job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform output: s3://sagemaker-us-east-2-214666064132/trends-styles-important-basic-2019-11-2-2019-11-24-17-19-47-932\n"
     ]
    }
   ],
   "source": [
    "# Create model from model package\n",
    "model = sm.ModelPackage(\n",
    "            role=sm.get_execution_role(),\n",
    "            model_package_arn=model_package_arn,\n",
    "            sagemaker_session=sm.Session())\n",
    "\n",
    "# Create the transformer\n",
    "# A variety of parameters may be specified here including the output path where\n",
    "# SageMaker will send the results of the transform. Since we do not specify the output,\n",
    "# Sagemaker will leave the results in the default bucket. We will retrieve this location \n",
    "# below so that we can inspect the output.\n",
    "transformer = model.transformer(1, 'ml.m4.xlarge')\n",
    "\n",
    "# Run the transform job.\n",
    "# By default, the output file contains only the inference result for each row.\n",
    "# You can use the output_filter parameter to include any of the input columns. Review also\n",
    "# input_filter which allows you to filter the parameters passed as input to the transformer.\n",
    "# The combination of input_filter and output_filter gives you a lot of flexibility.\n",
    "# By default, the API does not wait for the transform job to complete. You can control this with\n",
    "# the wait parameter.\n",
    "transformer.transform(transform_input, content_type='text/csv')\n",
    "\n",
    "# The transform job sets the output path in the output_path member.\n",
    "print(f'Transform output: {transformer.output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
